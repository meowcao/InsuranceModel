# ä¿é™©çŸ¥è¯†é—®ç­”åŠ©æ‰‹

## ä»‹ç»

â€‹	æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„ä¿é™©çŸ¥è¯†é—®ç­”å¤§è¯­è¨€æ¨¡å‹ï¼è¿™ä¸ªæ¨¡å‹æ˜¯åŸºäº`ChineseNlpCorpus`æä¾›çš„ä¸°å¯Œä¿é™©é¢†åŸŸæ•°æ®é›†å¼€å‘è€Œæˆã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ¶µç›–äº†ç”¨æˆ·çš„å„ç§ä¿é™©ç›¸å…³æé—®ã€ç½‘å‹çš„å›ç­”ä»¥åŠæœ€ä½³å›ç­”ï¼Œæ—¨åœ¨ä¸ºæ‚¨æä¾›å…¨é¢ã€å‡†ç¡®çš„ä¿é™©çŸ¥è¯†è§£ç­”ã€‚

â€‹	æˆ‘ä»¬çš„æ¨¡å‹æ˜¯æ‚¨åœ¨ä¿é™©é¢†åŸŸçš„æ™ºå›Šå›¢ï¼Œå…·å¤‡ä¸°å¯Œçš„ä¸“ä¸šçŸ¥è¯†å’Œåº”ç”¨èƒ½åŠ›ã€‚æ— è®ºæ‚¨æ˜¯æƒ³äº†è§£ä¿é™©çš„åŸºæœ¬æ¦‚å¿µã€ä¸åŒç±»å‹çš„ä¿é™©ï¼Œè¿˜æ˜¯éœ€è¦æŒ‡å¯¼ç†èµ”æµç¨‹æˆ–é€‰æ‹©åˆé€‚çš„ä¿é™©æ”¿ç­–ï¼Œæˆ‘ä»¬éƒ½èƒ½ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚

å…·ä½“å¦‚ä½•å®ç°å…¨æµç¨‹çš„ chat-AI å¾®è°ƒï¼Œå¯å‚è€ƒæœ¬ä»“åº“[meowcao/InsuranceModel: åŸºäºinternlm-chat-7bçš„ä¿é™©çŸ¥è¯†å¤§æ¨¡å‹å¾®è°ƒ (github.com)](https://github.com/meowcao/InsuranceModel?tab=readme-ov-file)

å¦‚ä½•å­¦ä¹ å¤§æ¨¡å‹éƒ¨ç½²å’Œå¾®è°ƒè¯·å‚è€ƒï¼š[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å—](https://github.com/datawhalechina/self-llm.git) ä»¥åŠ [ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹å®æˆ˜è¥è¯¾ç¨‹](https://github.com/InternLM/tutorial.git)

![structure](imgs/structure.png)

## OpenXlab æ¨¡å‹

ä¿é™©çŸ¥è¯†é—®ç­”åŠ©æ‰‹ä½¿ç”¨çš„æ˜¯InternLM çš„ 7B æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°é‡ä¸º 7Bï¼Œæ¨¡å‹å·²ä¸Šä¼ ï¼Œæ¨¡å‹å‚æ•°è¯¦è§`configs`ç›®å½•ã€‚



## æ•°æ®é›†

â€‹	ä¿é™©çŸ¥è¯†é—®ç­”åŠ©æ‰‹æ•°æ®é›†é‡‡ç”¨ä¸­çš„`ChineseNlpCorpus`æä¾›çš„åŒ…æ‹¬ç”¨æˆ·æé—®ã€ç½‘å‹å›ç­”ã€æœ€ä½³å›ç­”ï¼Œå…±è®¡ 588000 ä½™æ¡ï¼Œæ•°æ®é›†æ ·ä¾‹ï¼š

```
"input": "æœ€è¿‘åœ¨å®‰é‚¦é•¿é’æ ‘ä¸­çœ‹åˆ°ä»€ä¹ˆè±å…ï¼Œè¿™ä¸ªæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ"
"output": "æ‚¨å¥½ï¼Œè¿™ä¸ªæ˜¯é‡ç–¾é™©ä¸­ç»™äºˆæŠ•ä¿è€…çš„ä¸€é¡¹æƒåˆ©ï¼Œå®‰*é•¿é’æ ‘ä¿éšœè´£ä»»è§„å®šï¼ŒæŠ•ä¿è€…å¯ä»¥äº«å—å¤šæ¬¡èµ”ä»˜ï¼Œè±å…ç­‰æƒç›Šã€‚ä¹Ÿå°±æ˜¯è¯´ä¸åŒè½»ç—‡ç´¯è®¡5æ¬¡èµ”ä»˜ï¼Œç†èµ”1æ¬¡è½»ç—‡è±å…åæœŸæ‰€äº¤ä¿è´¹ï¼Œäººæ€§åŒ–çš„è®¾è®¡ï¼Œæ— éœ€åŠ ä¿è´¹ã€‚"
"input": "å’Œå›¢é˜Ÿå»åŒ—ææ¢é™©ï¼Œæœ‰æ²¡æœ‰é’ˆå¯¹è¿™æ–¹é¢çš„HUTSä¿é™©å‘¢"
"output": "æ‚¨å¥½ï¼Œå»åŒ—ææ¢é™©æœ¬èº«å°±å­˜åœ¨ä¸€å®šçš„é£é™©ï¼Œå»ºè®®é€‰æ‹©ä¸“ä¸šçš„è£…å¤‡ä»¥åŠåœ¨ä¸“ä¸šäººå£«çš„é™ªåŒä¸‹è¿›è¡Œã€‚è‡³äºä¿é™©ï¼Œå¸‚é¢ä¸Šå…³äºæ­¤ç±»çš„ä¿é™©å¹¶ä¸å¤šï¼Œä¸è¿‡HUTSä¿é™©ä¸­å´æœ‰ä¸€æ¬¾ä¸“é—¨é’ˆå¯¹å—åŒ—ææ—…æ¸¸çš„å®šåˆ¶äº§å“ï¼Œä¿éšœå†…å®¹å……è¶³ï¼Œæˆ·å¤–ä¼¤å®³ã€åŒ»ç–—ä¿éšœç”šè‡³çš„ç´§æ€¥æ•‘æ´éƒ½å…·å¤‡ï¼Œè¯¦æƒ…å¯ä»¥å¤šäº†è§£ä¸‹ã€‚"
```



### æ•°æ®å¤„ç†ä¸æ•´ç†

1. æ•°æ®é›†æ˜¯ä»¥CSVæ ¼å¼å­˜å‚¨çš„ï¼Œç¬¬ä¸€è¡Œä¸ºåˆ—åï¼Œåˆ†åˆ«ä¸º `title`, `reply`, `is_best`ã€‚
2. æ•°æ®é›†æŠŠæ•°æ®åˆ†ç±»ä¸ºä¼˜è´¨å›ç­”ï¼ˆis_best=1ï¼‰ä¸åŠ£è´¨å›ç­”ï¼ˆis_best=0ï¼‰ï¼Œéœ€è¦è¿‡æ»¤æ‰ `is_best` åˆ—ä¸º0çš„æ•°æ®ã€‚

ä½¿ç”¨å¦‚ä¸‹è„šæœ¬æ–‡ä»¶

```python
import csv
import json

def convert_csv_to_json(input_file, output_file):
    conversations = []
    
    with open(input_file, 'r', newline='', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        
        for row in reader:
            if int(row['is_best']) == 1:  # åªå¤„ç† is_best ä¸º 1 çš„æ•°æ®
                conversation = {
                    "system": "å½“ä½ å‘æˆ‘è¯¢é—®æœ‰å…³ä¿é™©çš„é—®é¢˜æ—¶ï¼Œè¯·æ”¾å¿ƒï¼Œæˆ‘æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿é™©çŸ¥è¯†é—®ç­”ä¸“å®¶ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨æ¾„æ¸…ä¿é™©çš„åŸºæœ¬æ¦‚å¿µå’Œä½œç”¨ï¼Œè§£é‡Šä¸åŒç±»å‹ä¿é™©çš„ç‰¹ç‚¹ï¼Œä»¥åŠå›ç­”å…³äºä¿é™©åˆåŒã€ç†èµ”æµç¨‹æˆ–ä¿é™©æ”¿ç­–çš„ä»»ä½•é—®é¢˜ã€‚æ‚¨å¯ä»¥éšæ—¶å‘æˆ‘æå‡ºå…³äºå¥åº·ã€è½¦è¾†ã€æˆ¿å±‹å’Œäººå¯¿ä¿é™©ç­‰æ–¹é¢çš„ç–‘é—®ã€‚å¦‚æœæ‚¨æƒ³äº†è§£å¦‚ä½•æå‡ºä¿é™©ç†èµ”ç”³è¯·æˆ–éœ€è¦å…³äºä¿é™©ç†èµ”æµç¨‹çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å°†ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„æŒ‡å¯¼å’Œå»ºè®®ã€‚",
                    "input": row['title'],
                    "output": row['reply']
                }
                conversations.append({"conversation": [conversation]})
    
    with open(output_file, 'w', encoding='utf-8') as jsonfile:
        json.dump(conversations, jsonfile, ensure_ascii=False, indent=4)

# æŒ‡å®šè¾“å…¥çš„CSVæ–‡ä»¶å’Œè¾“å‡ºçš„JSONæ–‡ä»¶
input_csv_file = 'D:\\Learning\\å·¥ç¨‹å®è®­\\baoxianzhidao_filter.csv'
output_json_file = 'D:\\Learning\\å·¥ç¨‹å®è®­\\output.json'

# è°ƒç”¨å‡½æ•°è¿›è¡Œè½¬æ¢
convert_csv_to_json(input_csv_file, output_json_file)

print(f"è½¬æ¢å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {output_json_file}")

```

- `convert_csv_to_json` å‡½æ•°è´Ÿè´£è¯»å–CSVæ–‡ä»¶ï¼Œè¿‡æ»¤å’Œè½¬æ¢æ•°æ®ï¼Œå¹¶å°†ç»“æœå†™å…¥JSONæ–‡ä»¶ã€‚
- æˆ‘ä»¬ä½¿ç”¨ `csv.DictReader` æ¥è¯»å–CSVæ–‡ä»¶ï¼Œå¹¶ä»¥å­—å…¸å½¢å¼è®¿é—®æ¯ä¸€è¡Œçš„æ•°æ®ã€‚
- å¯¹äºæ¯ä¸€è¡Œæ•°æ®ï¼Œæ ¹æ® `is_best` çš„å€¼å†³å®šæ˜¯å¦æ·»åŠ åˆ°æœ€ç»ˆçš„JSONè¾“å‡ºä¸­ã€‚
- æœ€åï¼Œä½¿ç”¨ `json.dump` å°†è½¬æ¢åçš„æ•°æ®å†™å…¥åˆ°æŒ‡å®šçš„JSONæ–‡ä»¶ä¸­ã€‚



## å¾®è°ƒ

â€ƒâ€ƒä½¿ç”¨ `XTuner`è®­ç»ƒï¼Œ `XTuner`æœ‰å„ä¸ªæ¨¡å‹çš„ä¸€é”®è®­ç»ƒè„šæœ¬ï¼Œå¾ˆæ–¹ä¾¿ã€‚ä¸”å¯¹` InternLM2 `çš„æ”¯æŒåº¦æœ€é«˜ã€‚

### XTuner

â€ƒâ€ƒä½¿ç”¨ `XTuner` è¿›è¡Œå¾®è°ƒï¼Œå…·ä½“è„šæœ¬å¯å‚è€ƒ`configs`æ–‡ä»¶å¤¹ä¸‹çš„è„šæœ¬ï¼Œè„šæœ¬å†…æœ‰è¾ƒä¸ºè¯¦ç»†çš„æ³¨é‡Šã€‚
	`Xtuner` çš„è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹æœ¬ç›®å½•ä¸‹çš„`XTUNERåŠå…¶å¾®è°ƒåŸç†ä»‹ç».md` ã€‚

| åŸºåº§æ¨¡å‹          | é…ç½®æ–‡ä»¶                               |
| ----------------- | -------------------------------------- |
| internlm-chat-7b  | internlm_chat_7b_qlora_medqa2019_e3.py |
| internlm2-chat-7b | internlm2_1_8b_qlora_alpaca_e3_copy.py |

å¾®è°ƒæ–¹æ³•å¦‚ä¸‹:

1. æ ¹æ®åŸºåº§æ¨¡å‹å¤åˆ¶ä¸Šé¢çš„é…ç½®æ–‡ä»¶ï¼Œå°†æ¨¡å‹åœ°å€`pretrained_model_name_or_path`å’Œæ•°æ®é›†åœ°å€`data_path`ä¿®æ”¹æˆè‡ªå·±çš„

```
conda activate xtuner0.1.9
cd ~/ft-medqa
xtuner train  internlm_chat_7b_qlora_medqa2019_e3.py --deepspeed deepspeed_zero2
```

2. å°†å¾—åˆ°çš„ `PTH` æ¨¡å‹è½¬æ¢ä¸º`HuggingFace`æ¨¡å‹

```
internlm_chat_7b_qlora_medqa2019_e3
xtuner convert pth_to_hf ./internlm_chat_7b_qlora_medqa2019_e3.py ./work_dirs/internlm_chat_7b_qlora_medqa2019_e3/epoch_1.pth ./hf
xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
```

### Chat

```
xtuner chat ./merged --prompt-template internlm_chat
```

## æœ¬åœ°ç½‘é¡µéƒ¨ç½²

```
streamlit run /root/ft-medqa/code/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

æ•ˆæœæ¼”ç¤º

![local_3](imgs/local_3.png)

## OpenXLab éƒ¨ç½² ä¸­åŒ»è¯çŸ¥è¯†é—®ç­”åŠ©æ‰‹

### 1 ä¸Šä¼ æ¨¡å‹

æŒ‰ç…§æ•™ç¨‹å†…å®¹å®‰è£…å¹¶é…ç½®gitï¼Œåœ¨openxlabä¸­æ–°å»ºæ¨¡å‹ä»“åº“ï¼Œå¹¶ä¸Šä¼ æ¨¡å‹æ–‡ä»¶ã€‚

ä»“åº“ç›®å½•å¦‚ä¸‹ï¼š

```
â”œâ”€insurance
â”‚  â”œâ”€.gitattributes                 
â”‚  â”œâ”€README.md       
â”‚  â”œâ”€config.json           
|  â”œâ”€configuration_internlm2.py  
|  â”œâ”€generation_config.json 
|  â”œâ”€modeling_internlm2.py 
|  â”œâ”€pytorch_model-00001-of-00002.bin 
|  â”œâ”€pytorch_model-00002-of-00002.bin 
|  â”œâ”€pytorch_model.bin.index.json
|  â”œâ”€special_tokens_map.json
|  â”œâ”€tokenization_internlm2.py
|  â”œâ”€tokenization-special_internlm2.py
|  â”œâ”€tokenizer.json  
|  â”œâ”€tokenizer.model 
â”‚  â””â”€tokenizer_config.json
```

### 2 åˆå§‹åŒ–é¡¹ç›®ç»“æ„

åˆ›å»ºä¸€ä¸ªæ–°çš„ GitHub ä»“åº“æ¥å­˜æ”¾æ‚¨çš„ gradio åº”ç”¨ä»£ç ã€‚é¡¹ç›®ç»“æ„å¦‚ä¸‹ï¼š

```
â”œâ”€InsuranceLM
â”‚  â”œâ”€app.py                 # Gradio åº”ç”¨é»˜è®¤å¯åŠ¨æ–‡ä»¶ä¸ºapp.pyï¼Œåº”ç”¨ä»£ç ç›¸å…³çš„æ–‡ä»¶åŒ…å«æ¨¡å‹æ¨ç†ï¼Œåº”ç”¨çš„å‰ç«¯é…ç½®ä»£ç 
â”‚  â”œâ”€requirements.txt       # å®‰è£…è¿è¡Œæ‰€éœ€è¦çš„ Python åº“ä¾èµ–ï¼ˆpip å®‰è£…ï¼‰
â”‚  â”œâ”€packages.txt           # å®‰è£…è¿è¡Œæ‰€éœ€è¦çš„ Debian ä¾èµ–é¡¹ï¼ˆ apt-get å®‰è£…ï¼‰
|  â”œâ”€README.md              # ç¼–å†™åº”ç”¨ç›¸å…³çš„ä»‹ç»æ€§çš„æ–‡æ¡£
â”‚  â””â”€...
```

### 3 éƒ¨ç½²åº”ç”¨
åœ¨å¹³å°å†…æ–°å»ºgradioç»„ä»¶åº”ç”¨å¹¶å¯åŠ¨ã€‚



## LmDeployéƒ¨ç½²

### 1 ç¯å¢ƒé…ç½®

åŸºç¡€ç¯å¢ƒé…ç½®ï¼š

```bash
$ /root/share/install_conda_env_internlm_base.sh lmdeploy
$ conda activate lmdeploy
pip install packaging
pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
pip install lmdeploy
```



### 2 æœåŠ¡éƒ¨ç½²


æ¨¡å‹è½¬æ¢ï¼š


ç¦»çº¿è½¬æ¢éœ€è¦åœ¨å¯åŠ¨æœåŠ¡ä¹‹å‰ï¼Œå°†æ¨¡å‹è½¬ä¸º lmdeploy TurboMind  çš„æ ¼å¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```bash
lmdeploy convert internlm-chat-7b /path/to/internlm-chat-7b
```


æ‰§è¡Œå®Œæˆåå°†ä¼šåœ¨å½“å‰ç›®å½•ç”Ÿæˆä¸€ä¸ª `workspace` çš„æ–‡ä»¶å¤¹ã€‚è¿™é‡Œé¢åŒ…å«çš„å°±æ˜¯ TurboMind å’Œ Triton â€œæ¨¡å‹æ¨ç†â€éœ€è¦åˆ°çš„æ–‡ä»¶ã€‚



æ¨¡å‹è½¬æ¢å®Œæˆåæˆ‘ä»¬å…ˆå°è¯•æœ¬åœ°å¯¹è¯ï¼ˆ`Bash Local Chat`ï¼‰ï¼Œæ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ã€‚

```bash
# Turbomind + Bash Local Chat
lmdeploy chat turbomind ./workspace
```

å¯åŠ¨åå°±å¯ä»¥å’Œå®ƒè¿›è¡Œå¯¹è¯äº†ã€‚

è¾“å…¥åä¸¤æ¬¡å›è½¦ï¼Œé€€å‡ºæ—¶è¾“å…¥`exit` å›è½¦ä¸¤æ¬¡å³å¯ã€‚æ­¤æ—¶ï¼ŒServer å°±æ˜¯æœ¬åœ°è·‘èµ·æ¥çš„æ¨¡å‹ï¼ˆTurboMindï¼‰ï¼Œå‘½ä»¤è¡Œå¯ä»¥çœ‹ä½œæ˜¯å‰ç«¯ã€‚



## Lmdeploy&opencompass é‡åŒ–ä»¥åŠé‡åŒ–è¯„æµ‹

### `KV Cache`é‡åŒ–

åº”ç”¨ç¤ºä¾‹ï¼š
```python
from lmdeploy import pipeline, TurbomindEngineConfig
engine_config = TurbomindEngineConfig(quant_policy=8)
pipe = pipeline("internlm/internlm2-chat-7b", backend_config=engine_config)
response = pipe(["Hi, pls intro yourself", "Shanghai is"])
print(response)
```


### `W4A16`é‡åŒ–

æ§åˆ¶å°è¾“å…¥ä»¥ä¸‹æŒ‡ä»¤ï¼š

```bash
export HF_MODEL=insurance/final_model/model2
export WORK_DIR=work_dir/insurancelm

lmdeploy lite auto_awq \
   $HF_MODEL \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 2048 \
  --w-bits 4 \
  --w-group-size 128 \
  --work-dir $WORK_DIR
```

### é—®é¢˜

ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ¨¡å‹æ˜¯InternLM-chat-1_8bï¼Œè€Œä¸¤ç§é‡åŒ–æ–¹å¼ä»…æ”¯æŒå°†æ¨¡å‹é‡åŒ–è‡³4bæˆ–8bï¼Œæ•…åœ¨æ‰§è¡ŒæŒ‡ä»¤çš„è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ã€‚

```bash
Traceback (most recent call last):
  File "/root/.conda/envs/lmdeploy/bin/lmdeploy", line 8, in <module>
    sys.exit(run())
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/entrypoint.py", line 37, in run
    args.run(args)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/cli/lite.py", line 131, in auto_awq
    auto_awq(**kwargs)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/apis/auto_awq.py", line 69, in auto_awq
    quant_weights(model, fcs, w_bits, w_sym, w_group_size, device)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/quantization/awq.py", line 216, in quant_weights
    quantizer = WeightQuantizer(bits, symmetry, 'per_group', group_size)
  File "/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/quantization/weight/quantizer.py", line 67, in __init__
    assert bits in [4, 8], "The 'bits' argument must be either 4 or 8."
AssertionError: The 'bits' argument must be either 4 or 8.
```

è€ƒè™‘åˆ°æˆ‘ä»¬ä½¿ç”¨çš„1.8bæ¨¡å‹è§„æ¨¡å·²ç»è¾ƒå°ï¼Œåœ¨ä¸»æµçš„è®¾å¤‡ä¸Šä¹Ÿè¶³ä»¥æ”¯æŒï¼Œæ•…ä¸å†è¿›è¡Œé‡åŒ–æ“ä½œã€‚



## OpenCompass è¯„æµ‹

### 1. é¢å‘GPUçš„ç¯å¢ƒå®‰è£…

```bash
git clone https://github.com/open-compass/opencompass
cd opencompass
pip install -e .
```

### 2. ä¸‹è½½è§£å‹æ•°æ®é›†

```
cp /share/temp/datasets/OpenCompassData-core-20231110.zip /root/opencompass/
unzip OpenCompassData-core-20231110.zip
```

### 3. å¯åŠ¨è¯„æµ‹

```
export MKL_SERVICE_FORCE_INTEL=1
python run.py --datasets ceval_gen --hf-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-path /share/temp/model_repos/internlm-chat-7b/ --tokenizer-kwargs padding_side='left' truncation='left' trust_remote_code=True --model-kwargs trust_remote_code=True device_map='auto' --max-seq-len 2048 --max-out-len 16 --batch-size 4 --num-gpus 1 --debug
```

### 4. ä¿ç•™ç›¸å…³ç»“æœ

```
ceval-legal_professional: {'accuracy': 47.82608695652174}
```

å…¨éƒ¨è¯„æµ‹ç»“æœå¯åœ¨ç›®å½•`opencompassResult`ä¸­æŸ¥çœ‹ã€‚



## å‚è€ƒèµ„æ–™

1. EmoLLM-å¿ƒç†å¥åº·å¤§æ¨¡å‹ - https://github.com/SmartFlowAI/EmoLLM
2. Chat-å¬›å¬›-ç”„å¬›è§’è‰²æ‰®æ¼”å¤§æ¨¡å‹ - https://github.com/KMnO4-zx/xlab-huanhuan
3. ä¸­æ–‡ä¿é™©é—®ç­”æ•°æ®é›† https://github.com/SophonPlus/ChineseNlpCorpus
4. ä¹¦ç”Ÿæµ¦é›¨å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒè¥ https://github.com/InternLM/tutorial.git



## ğŸ’• è‡´è°¢

### é¡¹ç›®æˆå‘˜

* ä¿æ›¹-é¡¹ç›®è´Ÿè´£äºº ï¼ˆè´Ÿè´£é¡¹ç›®è§„åˆ’ï¼Œæ•°æ®æ¸…æ´—åŠæ¨¡å‹è®­ç»ƒï¼‰
* èŒƒå¾ç«‹ï¼ˆè´Ÿè´£æ•°æ®é›†æ”¶é›†ã€æ¨¡å‹è®­ç»ƒï¼‰
* é™ˆåšè¿œï¼ˆè´Ÿè´£æ¨¡å‹è¯„æµ‹ã€æ¨¡å‹éƒ¨ç½²ï¼‰
* å­™éŸ¬å¼ºï¼ˆè´Ÿè´£æ•°æ®é›†æ”¶é›†ã€æ¨¡å‹é‡åŒ–ï¼‰
* é‚¹é”¦å¸› ï¼ˆè´Ÿè´£æ•°æ®é›†æ”¶é›†ã€æ¨¡å‹é‡åŒ–ï¼‰



### ç‰¹åˆ«æ„Ÿè°¢

***æ„Ÿè°¢ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ç»„ç»‡çš„ ä¹¦ç”ŸÂ·æµ¦è¯­å®æˆ˜è¥ å­¦ä¹ æ´»åŠ¨***
